

\documentclass[conference]{IEEEtran}
\ifCLASSINFOpdf
   \usepackage[pdftex]{graphicx}
\else

\fi
\usepackage[cmex10]{amsmath}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{url}
\usepackage[caption=false,font=footnotesize]{subfig}
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{algpseudocode}


\begin{document}


\title{Interactive Online Learning for Obstacle Classification on a Mobile Robot}


\author{\IEEEauthorblockN{Viktor Losing\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
Barbara Hammer\IEEEauthorrefmark{1} and
Heiko Wersing\IEEEauthorrefmark{2}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Bielefeld University, Universit\"atsstr. 25, 33615 Bielefeld}
\IEEEauthorblockA{\IEEEauthorrefmark{2}HONDA Research Institute Europe, Carl-Legien-Str. 30, 63073 Offenbach am Main}
}



\maketitle



\begin{abstract}
Learning from non-stationary data streams is gaining more attention
recently, especially in the context of Internet of Things and Big Data.
It is a highly challenging task, since the fundamentally different types
of possibly occurring drift undermine classical assumptions such as
i.i.d data. Incremental drift characterizes a continuous change in the
distribution such as the signals of a slowly degrading sensor. A
suddenly malfunctioning sensor on the other hand causes a severe shift
and is defined as abrupt drift. Available algorithms are able to handle
different types of drift, however they target either abrupt or
incremental drift and often incorporate hyperparameter requiring a
priori knowledge about the task at hand.\\
We propose a biological inspired, architecture which partitions the data
into a short-term and long-term memory. The former is a window of
recently seen data-points whose size is adjusted such that the estimated
generalization error is minimized. The latter preserves only those
information from previous concepts which are non-conflicting to the
current one. These memories are combined according to the demands of the
present concept to classify unseen data points. We couple our parameter
free approach with the K-Nearest Neighbor classifier, however, any other
incremental learning algorithm could be used as well. \\
New artificial and real datasets are proposed to evaluate performance on
specific types of drift. Experiments on these as well as on generally
known benchmark datasets compare our approach with state of the art
methods. The highly competitive results throughout all experiments
underline the robustness of our approach.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Introduction}
\section{Concept Drift}
Definition and examples
\subsection{Types of Drift}
-abrupt, incremental, gradual, reoccuring, virtual
\section{Related Work}\label{relatedWork}
-Survey by gamma eventuell Ditzler
-Active drift detection work Gamma, Bifet ADWIN for abrupt to minimize delay
-Passive drift handling with ensembles DACC, LPPNSE
-Windowing but usually throws away information which still can be valid, no LTM
  -SVM Leave one out
  -KNNPAW 
-KNN for streaming like the ICDM paper


\section{Architecture}\label{IncrementaLearningMethod}
-STM, partitioning, O(log n) tests, maximizing generalization accuracy, write down in fancy formal way
-LTM, keep not compromising
-Transfer
-choosing of proper prediction model(STM, or LTM, or Both)
-Clustering
-Size management

\subsection{Efficiency}
-distances have to be calculated anyways once, can be stored and afterwards need only to be sorted

-fits with other algorithms as long they can be trained incremental and decremental e.g. NB, incremental SVM, but how to solve validation procedure

\section{Experiments}
We compare our method with well-known state of the art methods for handling drifting streaming data.
\\\textbf{Learn++.NSE with Classification and Regression Trees}\\
Proposed in \cite{5975223}, this algorithm processes incoming samples in chunks with a predefined size. A base classifier is trained for each chunk and added to the ensemble. The loss on recent chunks is averaged with a sigmoidal function
to compute the final weight of each base classifier. Similar to AdaBoost, instances are weighted such that misclassified inputs have a higher impact on the calculated loss.
Chunk-wise trained models have by design an adaption delay depending on the chunk size. The base classifier are in our case Classification and Regression Trees (CART).
\\\textbf{Leveraging bagging with Hoeffding Trees}\\
Proposed in \cite{bifet2010leveraging}, the authors suggest to increase the randomization of online bagging \cite{oza2005online} by a higher $\lambda$ value for the poisson distribution
and the usage of output detection codes. Additionaly, they use ADWIN as change detector for every classifier within the ensemble such that whenever a change is detected 
the worst classifier is replaced by a new one. As base classifier they use Hoeffding Trees with Gaussian Naive Bayes within the leaves. 
\\\textbf{Dynamic Adaption of Concept Changes with Hoeffding Trees}\\
Within this ensemble algorithm \cite{jaber2013online} a classifier of the worst half of the pool is removed randomly after a predefined number of examples and replaced by a new one. New examples are only
classified by the best classifier of the pool.
\\\textbf{kNN with Sliding Window}\\
This classifier is a kNN with a fixed window size containing the most recent samples. The sliding window is a standard approach for drift handling, since the window contains usually the most relevant examples
for the future predictions. However, the window can contain outdated examples interfering with the current concept leading to a deterioration of the prediction accuracy. 
\\\textbf{kNN with Probabilistic Adaptive Window and ADWIN}\\
In contrast to the approach with the sliding window, however, examples within the window are removed in random fashion leading to 

Table \ref{tab:algorithms} gives an overview of the algorithms as well as the chosen hyperparameter. 
\begin{table}
\def\arraystretch{0.9}
\footnotesize
\centering
\caption{The compared algorithms.}
\label{tab:algorithms}
\begin{tabular}{l|cccc}
\textit{Abbr.} & Classifier & Parameter\\\hline
HT\textsubscript{A} & Hoeffding Tree with ADWIN \\
LPPNSE & Learn++.NSE with CART& chunk-Size = various\\
DACC & Dynamic with HT& n=10\\
LVGB & Leveraging Bagging with HT& n=10\\
KNN\textsubscript{S} & KNN with sliding window & w=5000, k=5\\
KNN\textsubscript{W\textsubscript{A}} & NN with PAW+ADWIN& w=5000, k=5\\
KNN\textsubscript{M} &KNN with STM+LTM memory & w=5000, k=5\\
\end{tabular}
\label{tab:artDatasets}
\end{table}
A maximum of 5000 samples was allowed as size for the window based approaches. However,
we limited it to a maximum of 10\% of the whole dataset for those with only few examples. LPPNSE demands a chunk size which is critical for its performance. To avoid any disadvantage 
we evalueated several chunksizes and report the best achieved result. No further dataset specific hyperparameter tuning was done, as we wanted to use as little prior knowledge as possible.


\subsection{Datasets}
We used own and well known artificial as well as real world datasets to compare our method with state of the art algorithms. 
Links to all datasets as well as our own ones are available at \url{https://github.com/vlosing/Online-learning}. In the following we describe the data more detailed.\\

For the artificial data either published benchmarks were taken or generated with MOA using common parametrization in the literature.
We also added four new datasets allowing the evaluation of particular algorithm properties which are in our opinion not yet considered enough in the community. 
Table \ref{tab:artDatasets} shows their main characteristics.
\begin{table}
\def\arraystretch{0.9}
\footnotesize
\centering
\caption{Evaluated artificial datasets.}
\label{tab:datasets}
\begin{tabular}{l|cccc}
\textit{Dataset} & \#Samples&\#Feat.&\#Class&Drift type\\\hline
SEA Concepts & 50K & 3 & 2 & abrupt\\
Rotating Hyperplane & 200K & 10 & 2 & incremental\\
Moving RBF & 200K & 10 & 5 & incremental\\
Interchanging RBF & 200K & 2 & 10 & abrupt\\
Moving Squares & 200K & 2 & 4 & incremental\\
Transient Chessboard & 200K & 2 & 8 & virtual\\
Mixed Drift & 600K & 2 & 8 & abr/incr/virt\\
\end{tabular}
\label{tab:artDatasets}
\end{table}
\\\textbf{SEA Concepts (SEA)}\\
This two class dataset was proposed in \cite{Street:2001:SEA:502512.502568} and consists of 50000 samples with three attributes of which only two are relevant.
Abrupt drift is simulated with four different concepts, changing every 12500 samples, by using different thresholds $\theta_i$ such that $f_1 + f_2 > \theta_i$.
This dataset includes 10\% of noise.\\
\textbf{Rotating Hyperplane (HYP)}\\
A hyperplane in d-dimensional space is defined by the set of points $x$ that satisfy $\sum_{i=1}^{d}w_ix_i=w_0$. The position and orientation of the hyperplane
are changed incrementally by continuously adding a term $\delta$ to the weights $w_i=w_i+\delta$. 
We used the generator in MOA with the same parameters as in \cite{Bifet:2013:EDS:2480362.2480516} (10 dimensions, 2 classes, $\delta$=0.001).\\ 
\textbf{Moving RBF (MRBF)}\\
Gaussian distributions with random initial positions, weight and standard deviations are moved with constant speed $v$ in d-dimensional speed. 
The weight controls the partitioning of all samples among the Gaussians.
We used the generator in MOA with the same parameters as in \cite{Bifet:2013:EDS:2480362.2480516} (10 dimensions, 50 Gaussians, 5 classes, $v$=0.001).\\ 
\textbf{Interchanging RBF (IRBF)}\\
Ten Gaussians with random covariance matrix are exchanging positions every $2000$ samples and generate thereby a total of 100 abrupt drifts.\\ 
\textbf{Moving Squares (SQR)}\\
Four equidistantly seperated squares, representing different classes, are moving in horizontal direction with constant speed. The direction is inverted whenever the leading squares reaches a predefined boundary.
The nice property of this dataset is that the upper bound of stored recent examples such that old samples do not overlap current ones can be easily calculated (120) and  
facilitates the analysation of algorithms, especially those using a sliding window approach, for incremental drift.\\
\textbf{Transient Chessboard (CHESS)}
This dataset simulates virtual drift by succesively revealing random squares of a chessboard. Each time after four squares have been revealed, samples covering all squares of the board are shown.
facilitating the classification for algorithms which preserve as much information as possible.\\ 
\textbf{Mixed Drift (MIX)} \\
The datasets interchanging RBF, moving squares and transient chessboard datasets are simply positioned next to each other and samples belonging to one of these are introduced alternately.
Therefore, incremental, abrupt and virtual drift are taking place at the same time, requiring local adaptation to different drift types.\\

While the drift is explicitly generated in artificial datasets, it is rather difficult to identify the drift type in real world data or whether drift is present at all.
Unfortunately, only a few real world drift benchmarks are available, of which we used the largest ones. Additionaly, we contributed two challenging new datasets obtained from visual data.
The main attributes of all considered real world datasets are given in Table \ref{tab:realDatasets}.
\begin{table}
\def\arraystretch{0.9}
\footnotesize
\centering
\caption{Considered real world datasets.}
\label{tab:realDatasets}
\begin{tabular}{l|cccc}
\textit{Dataset} & \#Samples&\#Feat.&\#Class\\\hline
\rule{0pt}{8pt}
Weather & 18159 & 8 & 2\\
Elec & 27549 & 6 & 2\\
CovType & 581012 & 54 & 784\\
Outdoor & 4000 & 21 & 40\\
Railto & 40000 & 27 & 10\\
\end{tabular}
\label{tab:datasets}
\end{table}
\\\textbf{Weather}\\
Elwell et al. introduced this dataset in \cite{5975223}. Using eight different features such as temperature, pressure wind speed etc. the target is to predict 
whether it is going to rain on a certain day or not at the Offutt Air Force Base in Bellevue, Nebraska.
A period of 50 years is covered (1949-1999) summing up to 18159 samples with an imbalance towards no rain ($69\%$).\\
\textbf{Electricity market dataset}\\
This problem is often used as a benchmark for concept drift classification. Initially described in \cite{harries1999splice}, it was used thereafter for several performance comparisons \cite{baena2006early}, \cite{kuncheva2008adaptive}, \cite{Bifet:2013:EDS:2480362.2480516}, \cite{gama2004learning}. 
A critical note to its suitability as a benchmark can be found in \cite{zliobaite2013good}.
The dataset holds information of the Australian New South Wales Electricity Market, whose prices are affected by supply and demand. 
Each sample characterized by attributes such as day of week, timestamp, market demand etc. refers to a period of 30 minutes and the class label identifies the relative change (higher or lower) compared to the last 24 hours.
The dataset is often termed ELEC2 and contains 45312 samples. However, we removed those with missing values leading to a total of 27449 points.\\
\textbf{Forest Cover Type}\\
Assigns cartographic variables such as elevation, slope, soil type asf of $30 \times 30$ meter cells to different forest cover types. 
Only forests with minimal human-caused disturbances were used, so that resulting forest cover types are more a result of ecological processes.
It is often used as a benchmark for drift algorithms \cite{Bifet:2013:EDS:2480362.2480516}, \cite{gama2003accurate}, \cite{oza2001experimental}.\\
\textbf{Outdoor Objects}\\
This visual dataset was obtained from images recorded by a mobile robot approaching 40 different objects in a garden environment \cite{losing2015interactive}. The lighting conditions between 
the approaches are varying significantly caused by different weather conditions and/or cast shadows making the classification quite challenging. Each approach
consists of 10 images and is represented in temporal order within the dataset. Even though the representation is quite stable during a single approach, there is abrupt drift of varying degree between
approaches of the same object XXVL pictures. The objects are encoded in a normalized 21-dimensional rg-chromaticity histogram.\\
\textbf{Railto Bridge Timelapse}\\
Ten of the colorful buildings next to the famous rialto bridge in Venice are encoded in a normalized 27-dimensional rgb histogram. 
The images were obtained from timelapse videos of 10 consecutive days recorded in may 2016 by a webcam of XXVL. Incremental drift is caused by continuously changing weather conditions and
day times XXVL see figures. We excluded overnight recordings since they were too dark for being useful.

\subsection{Results}
We evaluated the algorithms in the standard online learning setting in which each incoming example is first classified and afterwards used for training. The error rates of all experiments are shown
in \ref{tab:result}.\\
\begin{table*}
\def\arraystretch{0.9}
\footnotesize
\centering
\caption{Error rates of all experiments evaluated with the Interleaved Test-Then-Train method.}
\label{tab:result}
\begin{tabular}{l|ccccccc}
Dataset & HT\textsubscript{A} & LPPNSE &DACC& LVGB & kNN\textsubscript{S} & kNN\textsubscript{W\textsubscript{A}}& kNN\textsubscript{M}\\\hline
SEA & 13.8 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
HYP & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\ 
MRBF & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
IRBF & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
SQR & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
CHESS & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
MIX & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
Art. avg & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
Art. rank & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
Weather & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
Elec & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
CovType & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
Outdoor & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\
Railto & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
Real avg& 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
Real rank& 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
total avg& 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
total rank& 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00 & 10.00\\\hline
\end{tabular}
\label{tab:datasets}
\end{table*}
Our approach outperforms the other methods quite significantly by having nearly half the error rate in average compared to the second best method LVGB. 
Even more important is in our eyes the fact that while all other methods are struggling at some datasets our method delivers very robust results without any hiccup. All drift types are handled
better or at least competitive compared to the other algorithms. This is particularly clarified in the large accuracy gap achieved with the MIX dataset, which contains incremental, 
abrupt and virtual drift at the same time. \\
The methodically most similar method to ours is kNN\textsubscript{W\textsubscript{A}}, since it uses also kNN as classifier and actively manages its window by sampling the input stream or clearing it in case of detected abrupt drift.
However, it performs in all experiments worse, even in those containing only abrupt drift.\\
Our results confirm the fact that kNN is in general a very competitive algorithm in the streaming setting. It is quite surprising that the simple sliding window approach of fixed window size 
performs comparably well or even better than more sophisticated methods such as HTAdaptive or L++.NSE.

-Table with results
-Evaluation
\subsection{Memory behaviour}
-STM-Size behaviour for SquaresIncr, rbfAbrupt
-Check prior behaviour for experiments
-Pictures of LTM for chess, rbfAbrupt


\section{Conclusion}

\newpage

% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://www.ctan.org/tex-archive/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,Drift}

% that's all folks
\end{document}

\iffalse
ToDo:

subfloats labeling Benni fragen
Test-Accuracy in Captions