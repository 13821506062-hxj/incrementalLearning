'''
Created on Dec 22, 2011

@author: jqueisse
'''

import scipy, numpy, time, scipy.misc
import pickle
from ImagePatch import ImagePatch
from svmutil import *
from ReliabilityMatrix import ReliabilityMatrix
import uuid
import DataIterator

import matplotlib.pyplot as pyplot

import libpythoninterface as libgxlvq

#from Tools import IPCA

class ControlLogic(object):
    '''
    classdocs
    '''


    def __init__(self):
        '''
        Constructor
        '''
        pass

    def doTrainStep(self, data):
        pass

    def doEvalStep(self, data):
        pass

    def finishTrain(self):
        pass

    #def finishEval(self, name):
    #    pass

    #def nextEval(self):
    #    pass

    def visualize(self, visoptions=None, save=None):
        pass

    def getVisOptions(self):
        pass

    def storeState(self, outfile):
        pass

    def restoreState(self, infile):
        pass

    def getExperimentInfo(self):
        pass

    def getName(self):
        return "none"

    def saveState(self, path):
        pass





class GXLVQclassifier(object):
    params=None

    traindata_len=0
    featurewidth=0

    traindata_collector=None
    traindata_labels=None


    def __init__(self, params):

        self.traindataframe=pyplot.figure();
        self.trainaxes=self.traindataframe.add_subplot(1, 1, 1)
        self.colors=["r","g","b"]
        self.samplecounter=0


        self.ignorablesamplecounter=0
        self.obstaclesamplecounter=0
        self.groundsamplecounter=0

        self.confusableignorablesamplecounter=0
        self.confusableobstaclesamplecounter=0
        self.confusablegroundsamplecounter=0


        #default config
        if not params.has_key("eigenvectors"): params["eigenvectors"]=10
        if not params.has_key("incremental"): params["incremental"]=True
        if not params.has_key("gxlvqmodelsavedir"): params["gxlvqmodelsavedir"]="/hri/localdisk/jqueisse/datasets/gxlvqmodels/"
        if not params.has_key("probability"): params["probability"]=False
        if not params.has_key("threeclasses"): params["threeclasses"]=False

        if not params.has_key("g_max"): params["g_max"]=40

        if not params.has_key("trainsteps"): params["trainsteps"]=10000
        if not params.has_key("nodesperclass"): params["nodesperclass"]=1
        if not params.has_key("randominit"): params["randominit"]=True

        self.params=params
        self.g_max = params["g_max"]

        self.featurewidth=params["eigenvectors"]
        self.threeclasses=params["threeclasses"]

        if params["probability"]==True:
            if not params.has_key("probability_history"): params["probability_history"]=10.0
            self.probhistory=params["probability_history"]

        self.probhelper=list()

        self.probability=params["probability"]



        #Create Modules:
        self.additionalwidth=0
        if params.has_key("contextwidth"):
            self.additionalwidth += params["contextwidth"]

        id = str(uuid.uuid1())
        self.net="net_" + id
        nettype=libgxlvq.NETTYPE_GMLVQ
        learnrate_per_node=libgxlvq.NETWORK_LEARNRATE
        self.num_of_classes=2
        if params["threeclasses"]==True:
            self.num_of_classes=3
        prototypes_per_class=params["nodesperclass"]
        trainsteps=params["trainsteps"]
        do_node_random_init=params["randominit"]
        threads_per_nodes=1
        self.dimensionality=self.featurewidth+self.additionalwidth
        initialdimensions = 0 # self.featurewidth

        libgxlvq.create_network(self.net, self.dimensionality, nettype, learnrate_per_node, self.num_of_classes,prototypes_per_class,trainsteps, do_node_random_init, threads_per_nodes, initialdimensions)



    def getParams(self):
        return self.params

    def printStatus(self):
        print "GXLVQ Status-> Prototypes: " , libgxlvq.get_numofprototypes_network(self.net)



    def doTrainStep(self, indata, inlabel, incremental_growth=True, percent=-1, confusable=0):

        label=-1
        if self.threeclasses==False:
            label=inlabel[0]
        else:
            if inlabel[0]>0:
                label=0
                self.groundsamplecounter+=1
                if confusable>0:
                    self.confusablegroundsamplecounter+=1
            elif inlabel[1]>0:
                label=1
                self.obstaclesamplecounter+=1
                if confusable>0:
                    self.confusableobstaclesamplecounter+=1
            elif inlabel[2]>0:
                label=2
                self.ignorablesamplecounter+=1
                if confusable>0:
                    self.confusableignorablesamplecounter+=1
            else:
                print "Error: no label found!"

        failinc=incremental_growth

        self.samplecounter = self.samplecounter+1

        if (self.samplecounter%10000)==0:
            dat = indata.flatten()
            #print dat
            self.trainaxes.plot(dat[0], dat[1], str(label+1)+self.colors[label])


        #Control signals:
        #percent
        netmode = libgxlvq.NETMODE_WEIGHTS
        if percent>=0:
            if percent<0.5:
                failinc=True
            else:
                failinc=False
            if percent>0.40:
                netmode = libgxlvq.NETMODE_BOTH

                #if (numpy.round(percent*10)%2)>0:
                #    netmode = libgxlvq.NETMODE_WEIGHTS
                #else:
                #    netmode = libgxlvq.NETMODE_LAMBDA





            else:
                netmode = libgxlvq.NETMODE_WEIGHTS


        #print "train online", indata.flatten(), [label], failinc
        [delta, perf] = libgxlvq.incremental_train_network(self.net, indata.flatten(), [label], failinc, self.g_max, netmode, False)


        if self.probability==True:
            [netres, index] = libgxlvq.process_network(self.net,indata.flatten())
            self.lastnodeindex=index[0];
            if self.threeclasses==False:
            #if not self.probability==True:
                outlabel = [netres[0], 1-netres[0], 0]
            else:
                #if not self.probability==True:
                if netres[0]<0.5:
                    outlabel = [1, 0, 0]
                elif netres[0]<1.5:
                    outlabel = [0, 1, 0]
                elif netres[0]<2.5:
                    outlabel = [0, 0, 1]
                else:
                    print "Error: no Label!"
            #print "prob:", self.probability, " dat:", outlabel, inlabel
            if ((outlabel[0]==inlabel[0])and(outlabel[1]==inlabel[1])):
                self.feedBack(1.0)
            else:
                self.feedBack(0.0)

        return perf

    def feedBack(self, performance=1.0):
        #print "feedback ", performance
        diff = (self.lastnodeindex - len(self.probhelper))+1
        if diff>0:
            self.probhelper.extend([1.0/self.num_of_classes]*diff)


        fac=1.0/self.probhistory

        #print self.lastnodeindex, len(self.probhelper), fac, performance, (1-fac), self.probhelper[self.lastnodeindex]
        self.probhelper[self.lastnodeindex]*=(1-fac)
        self.probhelper[self.lastnodeindex]+=performance*fac
        #print "probpast", self.lastnodeindex, self.probhelper[self.lastnodeindex]


    def doEvalStep(self, data):

        [result, index] = libgxlvq.process_network(self.net,data.flatten())
        #, (accuracy, mean_squared, correlation_coefficient)

        self.lastnodeindex=index[0];

        if self.threeclasses==False:
            #if not self.probability==True:
            res = [result[0], 1-result[0], 0]
            #else:
            #Bei 2-Klassen svm -> daten verdreht !
            #res = [probablility[0][1], probablility[0][0], 0]
        else:
            #if not self.probability==True:
            if result[0]<0.5:
                res = [1, 0, 0]
            elif result[0]<1.5:
                res = [0, 1, 0]
            elif result[0]<2.5:
                res = [0, 0, 1]
            else:
                print "Error: no Label!"
            #else:
            #res = [probablility[0][0], probablility[0][1], probablility[0][2]]
        #data


        if self.probability==True:
            for i in range(3):
                if len(self.probhelper)>self.lastnodeindex:
                    if res[i]>0.5:
                        res[i]*=self.probhelper[self.lastnodeindex]
                    else:
                        res[i]=(1-self.probhelper[self.lastnodeindex])/2
                else:
                    res[i]=1.0/3.0


        return res

    def finishTrain(self):

        protos = libgxlvq.get_numofprototypes_network(self.net)

        for p in range(protos):
            pos = libgxlvq.get_weights_network(self.net, p)
            labelnr = libgxlvq.get_prototypelabel_network(self.net, p)
            self.trainaxes.plot(pos[0],pos[1], "D"+self.colors[labelnr])


        print "samples:", self.samplecounter
        print "protos:", len(self.probhelper)
        print self.probhelper

        self.traindataframe.show()



    def storeState(self, outfile):
        #svm obj is not pickable:
        id = str(uuid.uuid1())
        path = self.params["gxlvqmodelsavedir"] + id + ".net"
        libgxlvq.save_network(self.net, path)

        pickle.dump(path, outfile, pickle.HIGHEST_PROTOCOL)
        pickle.dump(self.probhelper, outfile, pickle.HIGHEST_PROTOCOL)

    def restoreState(self, infile):
        path=pickle.load(infile)
        self.probhelper=pickle.load(infile)
        libgxlvq.delete_network(self.net)
        libgxlvq.load_network(self.net, path)


    def saveState(self, path, pos):

        lambdas = libgxlvq.get_lambdas_network(self.net,0)
        scipy.misc.toimage(numpy.abs(lambdas), cmin=0.0, cmax=1.0).save(path + str(pos) + "_lambdas.png", cmin=0, cmax=255)
        with open(path+"learnrates.txt", "a") as f:
            f.write(str(pos) + ":\n  Nodes(" + str(libgxlvq.get_numofprototypes_network(self.net)) + ")\n" )
            f.write("  obstacle samples: " + str(self.obstaclesamplecounter) + " (" + str(self.confusableobstaclesamplecounter) +   " confusable) \n")
            f.write("  ground samples: " + str(self.groundsamplecounter) + " (" + str(self.confusablegroundsamplecounter) +   " confusable) \n")
            f.write("  ignorable samples: " + str(self.ignorablesamplecounter) + " (" + str(self.confusableignorablesamplecounter) +   " confusable) \n")


            f.write("  Lambdas")
            for row in range(lambdas.shape[0]):
                f.write("\n    ")
                for col in range(lambdas.shape[1]):
                    f.write("%.4f " % lambdas[row,col])
            f.write("\n\n")






















class SVNclassifier(object):
    params=None

    traindata_len=0
    featurewidth=0

    traindata_collector=None
    traindata_labels=None


    def __init__(self, params):

        #default config
        if not params.has_key("eigenvectors"): params["eigenvectors"]=10
        if not params.has_key("membuffer"): params["membuffer"]=3000
        if not params.has_key("modelsavedir"): params["modelsavedir"]="/hri/localdisk/jqueisse/datasets/svmmodels/"
        if not params.has_key("probability"): params["probability"]=False
        if not params.has_key("threeclasses"): params["threeclasses"]=False
        if not params.has_key("svm_c"): params["svm_c"]=0.1
        if not params.has_key("svm_type_rbf"): params["svm_type_rbf"]=False
        if not params.has_key("svm_gamma"): params["svm_gamma"]=0.001

        self.params=params

        self.featurewidth=params["eigenvectors"]
        self.threeclasses=params["threeclasses"]

        if params["probability"]==True:
            self.svmoptions="-b 1"
        else:
            self.svmoptions=""
        self.probability=params["probability"]
        self.svm_c=params["svm_c"]
        self.svm_type_rbf= params["svm_type_rbf"]
        self.gamma=params["svm_gamma"]


        #Create Modules:
        self.additionalwidth=0
        if params.has_key("contextwidth"):
            self.additionalwidth += params["contextwidth"]

        self.traindata_collector=scipy.zeros((3000, self.featurewidth+self.additionalwidth))
        self.traindata_labels=scipy.zeros(3000)

        self.model=None



    def getParams(self):
        return self.params



    def doTrainStep(self, indata, inlabel):

        tic = time.time()


        if self.traindata_collector.shape[0]<self.traindata_len+1:
            '''
            Allocate Mem ;)
            '''
            self.traindata_collector = scipy.vstack((self.traindata_collector,scipy.zeros((self.params["membuffer"],self.featurewidth+self.additionalwidth))))
            self.traindata_labels = scipy.hstack((self.traindata_labels, scipy.zeros(self.params["membuffer"])))

        self.traindata_collector[self.traindata_len,:]=indata.flatten()
        if self.threeclasses==False:
            self.traindata_labels[self.traindata_len]=inlabel[0]
        else:
            if inlabel[0]>0:
                self.traindata_labels[self.traindata_len]=0
            elif inlabel[1]>0:
                self.traindata_labels[self.traindata_len]=1
            elif inlabel[2]>0:
                self.traindata_labels[self.traindata_len]=2
            else:
                print "Error: no label found!"


        self.traindata_len +=1


    def svm_predict_mod(self, y, x, m, options=""):
        """
        svm_predict(y, x, m [, "options"]) -> (p_labels, p_acc, p_vals)

        Predict data (y, x) with the SVM model m.
        "options":
            -b probability_estimates: whether to predict probability estimates,
                0 or 1 (default 0); for one-class SVM only 0 is supported.

        The return tuple contains
        p_labels: a list of predicted labels
        p_acc: a tuple including  accuracy (for classification), mean-squared
               error, and squared correlation coefficient (for regression).
        p_vals: a list of decision values or probability estimates (if '-b 1'
                is specified). If k is the number of classes, for decision values,
                each element includes results of predicting k(k-1)/2 binary-class
                SVMs. For probabilities, each element contains k values indicating
                the probability that the testing instance is in each class.
                Note that the order of classes here is the same as 'model.label'
                field in the model structure.
        """


        predict_probability = 0
        argv = options.split()
        i = 0
        while i < len(argv):
            if argv[i] == '-b':
                i += 1
                predict_probability = int(argv[i])
            else:
                raise ValueError("Wrong options")
            i+=1

        svm_type = m.get_svm_type()
        is_prob_model = m.is_probability_model()
        nr_class = m.get_nr_class()
        pred_labels = []
        pred_values = []

        if predict_probability:
            if not is_prob_model:
                raise ValueError("Model does not support probabiliy estimates")

            if svm_type in [svm.NU_SVR, svm.EPSILON_SVR]:
                print("Prob. model for test data: target value = predicted value + z,\n"
                "z: Laplace distribution e^(-|z|/sigma)/(2sigma),sigma=%g" % m.get_svr_probability());
                nr_class = 0

            prob_estimates = (c_double * nr_class)()
            for xi in x:
                xi, idx = gen_svm_nodearray(xi)
                label = libsvm.svm_predict_probability(m, xi, prob_estimates)
                values = prob_estimates[:nr_class]
                pred_labels += [label]
                pred_values += [values]
        else:
            if is_prob_model:
                print("Model supports probability estimates, but disabled in predicton.")
            if svm_type in (svm.ONE_CLASS, svm.EPSILON_SVR, svm.NU_SVC):
                nr_classifier = 1
            else:
                nr_classifier = nr_class*(nr_class-1)//2
            dec_values = (c_double * nr_classifier)()
            for xi in x:
                xi, idx = gen_svm_nodearray(xi)
                label = svm.libsvm.svm_predict_values(m, xi, dec_values)
                values = dec_values[:nr_classifier]
                pred_labels += [label]
                pred_values += [values]

        return pred_labels, pred_values


    def doEvalStep(self, data):
        (result, probablility)=self.svm_predict_mod([0], [data.tolist()], self.model, options=self.svmoptions)
        #, (accuracy, mean_squared, correlation_coefficient)
        #print "eval: ", result, probablility, data

        if self.threeclasses==False:
            if not self.probability==True:
                res = [result[0], 1-result[0], 0]
            else:
                #Bei 2-Klassen svm -> daten verdreht !
                res = [probablility[0][1], probablility[0][0], 0]
        else:
            if not self.probability==True:
                if result[0]<0.5:
                    res = [1, 0, 0]
                elif result[0]<1.5:
                    res = [0, 1, 0]
                elif result[0]<2.5:
                    res = [0, 0, 1]
                else:
                    print "Error: no Label!"
            else:
                res = [probablility[0][0], probablility[0][1], probablility[0][2]]
        #data
        return res

    def finishTrain(self):
        self.traindata_collector = self.traindata_collector[:self.traindata_len,:]
        self.traindata_labels = self.traindata_labels[:self.traindata_len]
        print "SVM Train:", self.traindata_collector.shape, " -- ", self.traindata_labels.shape
        problem = svm.svm_problem(self.traindata_labels.tolist(), self.traindata_collector.tolist())
        parameter = svm.svm_parameter()

        if self.svm_type_rbf==False:
            parameter.kernel_type=svm.LINEAR

        parameter.gamma=self.gamma
        parameter.C=self.svm_c

        if self.params["probability"]==True:
            parameter.probability=True

        self.model = svm_train(problem, parameter)



    def storeState(self, outfile):
        #svm obj is not pickable:
        id = str(uuid.uuid1())
        path = self.params["modelsavedir"] + id + ".p"
        svm_save_model(path, self.model)

        pickle.dump(path, outfile, pickle.HIGHEST_PROTOCOL)

    def restoreState(self, infile):
        path=pickle.load(infile)
        self.model = svm_load_model(path)


class ContextUtilisation(object):

    def __init__(self, params):
        contextwidth=0
        if not params.has_key("position"):
            params["position"]=False
        if not params.has_key("rotation"):
            params["rotation"]=False
        if not params.has_key("time"):
            params["time"]=False

        if params["position"]==True:
            contextwidth+=2
        if params["rotation"]==True:
            contextwidth+=2
        if params["time"]==True:
            contextwidth+=1

        params["contextwidth"]=contextwidth

        self.params=params

        self.contextNormalization=True
        self.performContextNormalization=False

        if self.contextNormalization==True:
            self.meanvec=numpy.zeros((contextwidth))
            self.variancevec=numpy.zeros((contextwidth))
            #self.alpha=0.9
            self.alpha=0.01
            self.alphafac=0.95

        if self.performContextNormalization==True:
            # contextvariance  [ 75.46705814  72.31806749   0.45473822   0.50373498]   contextmean: [  1.00094894e+01  -4.88537240e+00   4.81664733e-03  -2.03724333e-01]

            #contextvariance  [ 154.32733836   34.48099418    0.34496507    0.64646094]   contextmean: [ 10.08683233  -0.54195819  -0.03459718   0.08588797]

            #self.variancevec=[154.32733836, 34.48099418, 0.34496507, 0.64646094]
            #self.meanvec=[10.1, -0.54195819, -0.03459718, 0.08588797]

            #small dataset:
            self.variancevec=[142.6, 45.6, 0.5, 0.5]
            self.meanvec=[8.7, 0.009, 0.04, 0.05]


    def getContextVec(self, context, train=False):
        res=[]
        if self.params["position"]==True:

            #res = numpy.hstack((res, [context.pos[0]/25.0,context.pos[1]/25.0]))
            res = numpy.hstack((res, [context.pos[0],context.pos[1]]))
        if self.params["rotation"]==True:

            #res = numpy.hstack((res, [numpy.sin(context.roation)*2, numpy.cos(context.roation)*2]))
            res = numpy.hstack((res, [numpy.sin(context.roation), numpy.cos(context.roation)]))
        if self.params["time"]==True:
            res = numpy.hstack((res, context.time))


        if (self.contextNormalization==True) and (train==True):
            diff = res-self.meanvec
            incr = self.alpha * diff
            self.meanvec += incr
            self.variancevec = (1-self.alpha)*(self.variancevec + diff*incr)

            #self.alpha *= self.alphafac

        if self.contextNormalization==True and len(res)>0:
            res /= numpy.sqrt(self.variancevec)


        if self.performContextNormalization==True and len(res)>0:
            res -= self.meanvec
            res /= numpy.sqrt(self.variancevec)

            print "contextvariance ", self.variancevec, "  contextmean:", self.meanvec

        return res




class OfflineSVM(ControlLogic):
    params=None
    data_patcher=None

    traindata_len=0
    featurewidth=0

    offlineclassifier=None
    featuretransformation=None

    def __init__(self, params):

        #default config
        if not params.has_key("in_size"): params["in_size"]=(256,256)
        if not params.has_key("patchsize"): params["patchsize"]=5
        if not params.has_key("eigenvectors"): params["eigenvectors"]=10

        self.params=params

        self.featurewidth=params["eigenvectors"]

        #Create Modules:
        self.data_patcher=ImagePatch(params["patchsize"], params["in_size"])
        self.traindata_collector=scipy.zeros((3000, self.featurewidth))
        self.traindata_labels=scipy.zeros(3000)

        self.offlineclassifier=SVNclassifier(self.params)
        self.params = self.offlineclassifier.getParams()
        self.featuretransformation=FeatureExtraction(self.params)
        self.params = self.featuretransformation.getParams()


        self.fig=pyplot.figure()
        self.axes=list()
        self.figx=1
        self.figy=2

        for i in range (self.figx*self.figy):
            self.axes.append(self.fig.add_subplot(self.figy, self.figx, i))

        self.axes[0].set_ylim(*self.axes[0].get_ylim()[::-1])

    def doTrainStep(self, data, ground_truth, mask):

        tic = time.time()

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patches.fill(1)


        selectedPatches=0
        self.data_patcher.setImage(data.camImg)



        for p in range(self.data_patcher.getNrOfPatches()):

            cp = self.data_patcher.getPatchByNr(p)

            if not cp==None:
                selectedPatches+=1
                (xp,yp)=self.data_patcher.getPosByNr(p)
                self.offlineclassifier.doTrainStep(self.featuretransformation.doTransformation(cp), ground_truth.getReliability(xp, yp))
                self.traindata_len +=1

        print selectedPatches,"doTrainstep for all patches %.2ffps" %(1./(time.time()-tic))
        return selectedPatches

    def doEvalStep(self, data, mask):
        self.data_patcher.setImage(data.camImg)

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patchmask.fill(1)


        selectedPatches=0

        self.relmat = ReliabilityMatrix(self.data_patcher.getWidth(), self.data_patcher.getHeight(), startval=-1)
        for p in range(self.data_patcher.getNrOfPatches()):
            cp = self.data_patcher.getPatchByNr(p)
            (xp,yp)=self.data_patcher.getPosByNr(p)
            if not cp==None:
                selectedPatches+=1
                cp_transformed = self.featuretransformation.doTransformation(cp)
                result = self.offlineclassifier.doEvalStep(cp_transformed)
                self.relmat.setReliability(xp, yp, result)
            else:
                self.relmat.setReliability(xp, yp, [-1,-1,-1])
        #print "eigen", neigen, "Dist", dist


        return self.relmat

    def finishTrain(self):
        self.offlineclassifier.finishTrain()
        pass


    def visualize(self, visoptions=None, save=None):
        if not visoptions==None:
            if (visoptions.has_key("Classifier Result") and visoptions["Classifier Result"].rfind("true")>=0) or not save==None:
                self.axes[0].clear()
                self.relmat.visualize(axis=self.axes[0], pie=True)
                self.axes[0].set_title("Classifier Result")
            if (visoptions.has_key("Input Patches") and visoptions["Input Patches"].rfind("true")>=0) or not save==None:
                self.axes[1].clear()
                self.data_patcher.visualize(axis=self.axes[1])
                self.axes[1].set_title("Input Patches")
            if visoptions.has_key("PCA-Components") and visoptions["PCA-Components"].rfind("true")>=0:
                self.featuretransformation.visualize()

        self.fig.show()
        if not save==None:
            filename = save + "_" + self.getName() + ".png"
            self.fig.savefig(filename, transparent=True)



    def getVisOptions(self):
        return ["Classifier Result", "Input Patches", "Ground Truth", "Failed Patches", "PCA-Components"]

    def storeState(self, outfile):
        ptr = open(outfile, "wb" )
        self.offlineclassifier.storeState(ptr)
        ptr.close()


    def restoreState(self, infile):
        ptr = open(infile, "rb" )
        self.offlineclassifier.restoreState(ptr)
        ptr.close()

    def getExperimentInfo(self):
        res="Parameter:<ul>"

        for e in self.params:
            res += "<li>" + str(e) + " = " + str(self.params[e]) + "</li>"

        return res+"</lu></br>"

    def getName(self):
        return "offline_svm_lin"






class OfflineSVMContext(ControlLogic):
    params=None
    data_patcher=None

    traindata_len=0
    featurewidth=0

    offlineclassifier=None
    featuretransformation=None

    def __init__(self, params):

        #default config
        if not params.has_key("in_size"): params["in_size"]=(256,256)
        if not params.has_key("patchsize"): params["patchsize"]=5
        if not params.has_key("eigenvectors"): params["eigenvectors"]=10

        self.params=params

        self.featurewidth=params["eigenvectors"]

        #Create Modules:
        self.data_patcher=ImagePatch(params["patchsize"], params["in_size"])
        self.traindata_collector=scipy.zeros((3000, self.featurewidth))
        self.traindata_labels=scipy.zeros(3000)


        self.contextutilisation=ContextUtilisation(self.params)
        self.params=self.contextutilisation.params
        self.offlineclassifier=SVNclassifier(self.params)
        self.params = self.offlineclassifier.getParams()
        self.featuretransformation=FeatureExtraction(self.params)
        self.params = self.featuretransformation.getParams()


        self.fig=pyplot.figure()
        self.axes=list()
        self.figx=1
        self.figy=2

        for i in range (self.figx*self.figy):
            self.axes.append(self.fig.add_subplot(self.figy, self.figx, i))

        self.axes[0].set_ylim(*self.axes[0].get_ylim()[::-1])

    def doTrainStep(self, data, ground_truth, mask, percent=-1):

        tic = time.time()

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patches.fill(1)


        selectedPatches=0
        self.data_patcher.setImage(data.camImg)

        contextvec = self.contextutilisation.getContextVec(data);
        print "contextsize:",len(contextvec)
        print "Context: ", contextvec



        for p in range(self.data_patcher.getNrOfPatches()):

            cp = self.data_patcher.getPatchByNr(p)

            if not cp==None:
                selectedPatches+=1
                (xp,yp)=self.data_patcher.getPosByNr(p)

                if self.featurewidth>0:
                    cp_transformed = self.featuretransformation.doTransformation(cp)
                else:
                    cp_transformed=[]

                data = numpy.hstack((cp_transformed, contextvec))

                self.offlineclassifier.doTrainStep(data , ground_truth.getReliability(xp, yp))
                self.traindata_len +=1

        print selectedPatches,"doTrainstep for all patches %.2ffps" %(1./(time.time()-tic))
        return selectedPatches

    def doEvalStep(self, data, mask):
        self.data_patcher.setImage(data.camImg)

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patchmask.fill(1)


        selectedPatches=0

        contextvec = self.contextutilisation.getContextVec(data);
        print "Context: ", contextvec

        self.relmat = ReliabilityMatrix(self.data_patcher.getWidth(), self.data_patcher.getHeight(), startval=-1)
        for p in range(self.data_patcher.getNrOfPatches()):
            cp = self.data_patcher.getPatchByNr(p)
            (xp,yp)=self.data_patcher.getPosByNr(p)
            if not cp==None:
                selectedPatches+=1
                if self.featurewidth>0:
                    cp_transformed = self.featuretransformation.doTransformation(cp)
                else:
                    cp_transformed=[]
                data = numpy.hstack((cp_transformed, contextvec))
                #print "data:", data
                result = self.offlineclassifier.doEvalStep(data)
                if result[0]>result[1]:
                    if result[0]>result[2]:
                        result=[1,0,0]
                    else:
                        result=[0,0,1]
                else:
                    if result[1]>result[2]:
                        result=[0,1,0]
                    else:
                        result=[0,0,1]
                self.relmat.setReliability(xp, yp, result)
            else:
                self.relmat.setReliability(xp, yp, [-1,-1,-1])
        #print "eigen", neigen, "Dist", dist


        return self.relmat

    def finishTrain(self):
        self.offlineclassifier.finishTrain()
        pass


    def visualize(self, visoptions=None, save=None):
        if not visoptions==None:
            if (visoptions.has_key("Classifier Result") and visoptions["Classifier Result"].rfind("true")>=0) or not save==None:
                self.axes[0].clear()
                self.relmat.visualize(axis=self.axes[0], pie=True)
                self.axes[0].set_title("Classifier Result")
            if (visoptions.has_key("Input Patches") and visoptions["Input Patches"].rfind("true")>=0) or not save==None:
                self.axes[1].clear()
                self.data_patcher.visualize(axis=self.axes[1])
                self.axes[1].set_title("Input Patches")
            if visoptions.has_key("PCA-Components") and visoptions["PCA-Components"].rfind("true")>=0:
                self.featuretransformation.visualize()

        self.fig.show()
        if not save==None:
            filename = save + "_" + self.getName() + ".png"
            self.fig.savefig(filename, transparent=True)



    def getVisOptions(self):
        return ["Classifier Result", "Input Patches", "Ground Truth", "Failed Patches", "PCA-Components"]

    def storeState(self, outfile):
        ptr = open(outfile, "wb" )
        self.offlineclassifier.storeState(ptr)
        ptr.close()


    def restoreState(self, infile):
        ptr = open(infile, "rb" )
        self.offlineclassifier.restoreState(ptr)
        ptr.close()

    def getExperimentInfo(self):
        res="Parameter:<ul>"

        for e in self.params:
            res += "<li>" + str(e) + " = " + str(self.params[e]) + "</li>"

        return res+"</lu></br>"

    def getName(self):
        return "offline_svm_lin_context"










class OfflineGXLVQ(ControlLogic):
    params=None
    data_patcher=None

    traindata_len=0
    featurewidth=0

    offlineclassifier=None
    featuretransformation=None

    def __init__(self, params):

        #default config
        if not params.has_key("in_size"): params["in_size"]=(256,256)
        if not params.has_key("patchsize"): params["patchsize"]=5
        if not params.has_key("eigenvectors"): params["eigenvectors"]=10

        self.params=params

        self.featurewidth=params["eigenvectors"]

        #Create Modules:
        self.data_patcher=ImagePatch(params["patchsize"], params["in_size"])
        self.traindata_collector=scipy.zeros((3000, self.featurewidth))
        self.traindata_labels=scipy.zeros(3000)


        self.contextutilisation=ContextUtilisation(self.params)
        self.params=self.contextutilisation.params
        self.offlineclassifier=GXLVQclassifier(self.params)
        self.params = self.offlineclassifier.getParams()
        self.featuretransformation=FeatureExtraction(self.params, )
        self.params = self.featuretransformation.getParams()


        self.fig=pyplot.figure()
        self.axes=list()
        self.figx=1
        self.figy=2

        for i in range (self.figx*self.figy):
            self.axes.append(self.fig.add_subplot(self.figy, self.figx, i))

        self.axes[0].set_ylim(*self.axes[0].get_ylim()[::-1])

    def doTrainStep(self, data, ground_truth, mask, percent=-1):

        tic = time.time()

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patches.fill(1)


        selectedPatches=0
        self.data_patcher.setImage(data.camImg)

        contextvec = self.contextutilisation.getContextVec(data);
        print "contextsize:",len(contextvec)
        print "Context: ", contextvec



        for p in range(self.data_patcher.getNrOfPatches()):

            cp = self.data_patcher.getPatchByNr(p)

            if not cp==None:
                selectedPatches+=1
                (xp,yp)=self.data_patcher.getPosByNr(p)

                if self.featurewidth>0:
                    cp_transformed = self.featuretransformation.doTransformation(cp)
                else:
                    cp_transformed=[]

                data = numpy.hstack((cp_transformed, contextvec))

                self.offlineclassifier.doTrainStep(data , ground_truth.getReliability(xp, yp), percent=percent)
                self.traindata_len +=1

        self.offlineclassifier.printStatus()
        print selectedPatches,"doTrainstep for all patches %.2ffps" %(1./(time.time()-tic))
        return selectedPatches

    def doEvalStep(self, data, mask):
        self.data_patcher.setImage(data.camImg)

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patchmask.fill(1)


        selectedPatches=0

        contextvec = self.contextutilisation.getContextVec(data);
        print "Context: ", contextvec

        self.relmat = ReliabilityMatrix(self.data_patcher.getWidth(), self.data_patcher.getHeight(), startval=-1)
        for p in range(self.data_patcher.getNrOfPatches()):
            cp = self.data_patcher.getPatchByNr(p)
            (xp,yp)=self.data_patcher.getPosByNr(p)
            if not cp==None:
                selectedPatches+=1
                if self.featurewidth>0:
                    cp_transformed = self.featuretransformation.doTransformation(cp)
                else:
                    cp_transformed=[]
                data = numpy.hstack((cp_transformed, contextvec))
                #print "data:", data
                result = self.offlineclassifier.doEvalStep(data)
                if result[0]>result[1]:
                    if result[0]>result[2]:
                        result=[1,0,0]
                    else:
                        result=[0,0,1]
                else:
                    if result[1]>result[2]:
                        result=[0,1,0]
                    else:
                        result=[0,0,1]
                self.relmat.setReliability(xp, yp, result)
            else:
                self.relmat.setReliability(xp, yp, [-1,-1,-1])
        #print "eigen", neigen, "Dist", dist


        return self.relmat

    def finishTrain(self):
        self.offlineclassifier.finishTrain()
        pass


    def visualize(self, visoptions=None, save=None):
        if not visoptions==None:
            if (visoptions.has_key("Classifier Result") and visoptions["Classifier Result"].rfind("true")>=0) or not save==None:
                self.axes[0].clear()
                self.relmat.visualize(axis=self.axes[0], pie=True)
                self.axes[0].set_title("Classifier Result")
            if (visoptions.has_key("Input Patches") and visoptions["Input Patches"].rfind("true")>=0) or not save==None:
                self.axes[1].clear()
                self.data_patcher.visualize(axis=self.axes[1])
                self.axes[1].set_title("Input Patches")
            if visoptions.has_key("PCA-Components") and visoptions["PCA-Components"].rfind("true")>=0:
                self.featuretransformation.visualize()

        self.fig.show()
        if not save==None:
            filename = save + "_" + self.getName() + ".png"
            self.fig.savefig(filename, transparent=True)



    def getVisOptions(self):
        return ["Classifier Result", "Input Patches", "Ground Truth", "Failed Patches", "PCA-Components"]

    def storeState(self, outfile):
        ptr = open(outfile, "wb" )
        self.offlineclassifier.storeState(ptr)
        ptr.close()


    def restoreState(self, infile):
        ptr = open(infile, "rb" )
        self.offlineclassifier.restoreState(ptr)
        ptr.close()

    def getExperimentInfo(self):
        res="Parameter:<ul>"

        for e in self.params:
            res += "<li>" + str(e) + " = " + str(self.params[e]) + "</li>"

        return res+"</lu></br>"

    def getName(self):
        return "offline_gxlvq"






























class OnlineSystem1(ControlLogic):
    params=None
    data_patcher=None

    traindata_len=0
    featurewidth=0

    offlineclassifier=None
    onlineclassifier=None
    featuretransformation=None

    def __init__(self, params):

        #default config
        if not params.has_key("in_size"): params["in_size"]=(256,256)
        if not params.has_key("patchsize"): params["patchsize"]=5
        if not params.has_key("eigenvectors"): params["eigenvectors"]=10

        self.params=params

        self.featurewidth=params["eigenvectors"]

        #Create Modules:
        self.data_patcher=ImagePatch(params["patchsize"], params["in_size"])
        self.traindata_collector=scipy.zeros((3000, self.featurewidth))
        self.traindata_labels=scipy.zeros(3000)


        self.contextutilisation=ContextUtilisation(self.params)
        self.params=self.contextutilisation.params


        self.params["probability"]=True
        svm_props = self.params.copy()
        svm_props["threeclasses"]=False
        svm_props["position"]=False
        svm_props["rotation"]=False
        svm_props["time"]=False
        self.offlineclassifier=SVNclassifier(svm_props)
        offlinecfg = svm_props["offlinecl_data"]
        ptr = open(offlinecfg, "rb" )
        self.offlineclassifier.restoreState(ptr)
        ptr.close()


        self.onlineclassifier=GXLVQclassifier(self.params)
        self.params = self.onlineclassifier.getParams()
        self.featuretransformation=FeatureExtraction(self.params)
        self.params = self.featuretransformation.getParams()


        self.fig=pyplot.figure()
        self.axes=list()
        self.figx=1
        self.figy=2

        for i in range (self.figx*self.figy):
            self.axes.append(self.fig.add_subplot(self.figy, self.figx, i))

        self.axes[0].set_ylim(*self.axes[0].get_ylim()[::-1])



    def doTrainStep(self, data, ground_truth, mask, percent=-1, saveConsusables=None):

        tic = time.time()

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patches.fill(1)


        selectedPatches=0
        correctPatches=0
        self.data_patcher.setImage(data.camImg)

        contextvec = self.contextutilisation.getContextVec(data, train=True);
        print "contextsize:",len(contextvec)
        #print "Context: ", contextvec


        confusecounter=0
        for p in range(self.data_patcher.getNrOfPatches()):

            cp = self.data_patcher.getPatchByNr(p)

            if not cp==None:
                selectedPatches+=1
                (xp,yp)=self.data_patcher.getPosByNr(p)

                if self.featurewidth>0:
                    cp_transformed = self.featuretransformation.doTransformation(cp)
                else:
                    cp_transformed=[]

                data = numpy.hstack((cp_transformed, contextvec))




                ##Offline classifier:
                result_offline = self.offlineclassifier.doEvalStep(cp_transformed)
                result_offline[0]*=0.007
                result_offline[1]*=0.007
                result_offline[2]*=0.007
                result_online = self.onlineclassifier.doEvalStep(data)

                #print "compare : ", result_offline, result_online

                comb_result = numpy.vstack((result_offline,result_online))
                #print comb_result
                win_classifiernr=0
                tmpmax = numpy.max(comb_result,1)
                if tmpmax[1]>tmpmax[0]:
                    win_classifiernr=1

                win_class=[0,0]
                tmpmax = result_offline
                if tmpmax[0]<tmpmax[1] and tmpmax[1]>tmpmax[2]:
                    win_class[0]=1
                elif tmpmax[0]<tmpmax[2]:
                    win_class[0]=2
                tmpmax = result_online
                if tmpmax[0]<tmpmax[1] and tmpmax[1]>tmpmax[2]:
                    win_class[1]=1
                elif tmpmax[0]<tmpmax[2]:
                    win_class[1]=2


                #print "compareresult: ", win_classifiernr, win_class, comb_result

                trainsignal = ground_truth.getReliability(xp, yp)
                isConfusable = ground_truth.isConfusable[xp, yp]
                if trainsignal[win_class[win_classifiernr]]==1:
                    correctPatches=correctPatches+1

                #if win_classifiernr==1:
                    #print "Online Wins !!"



                if isConfusable>0 and trainsignal[1]>0.5:
                    confusecounter+=1


                if not saveConsusables==None:
                    if isConfusable>0:
                        #cp
                        #if trainsignal[1]>0:
                            #scipy.misc.toimage(cp).save(saveConsusables + "/obstacle_"+str(numpy.int(self.onlineclassifier.confusableobstaclesamplecounter))+".png")
                        #elif trainsignal[2]>0:
                            #scipy.misc.toimage(cp).save(saveConsusables + "/ignorable_"+str(numpy.int(self.onlineclassifier.confusableignorablesamplecounter))+".png")
                        #Train only confusables:
                        #self.onlineclassifier.doTrainStep(data , trainsignal, percent=percent, confusable = isConfusable)
                        pass

                self.onlineclassifier.doTrainStep(data , trainsignal, percent=percent, confusable = isConfusable)
                self.traindata_len +=1

                if (not trainsignal[win_class[win_classifiernr]]==1) or (win_classifiernr==1):
                    if win_classifiernr==0:
                        print "Fail OFFLINE !!"
                    #wrong result train classifier or online classifier selected:
                    #print "train online clasifier!"
                    #self.onlineclassifier.doTrainStep(data , trainsignal, percent=percent, confusable = isConfusable)
                    #self.traindata_len +=1
                else:
                    #no training just feedbak for actualization prob. estimation
                    if trainsignal[win_class[1]]==1:
                        #positive feedback
                        #print "positive feedback!"
                        #self.onlineclassifier.feedBack(1.0)
                        pass
                    else:
                        #negative feedback
                        #print "negative feedback!"
                        #self.onlineclassifier.feedBack(0.0)
                        pass


        #print "confusecounter=",confusecounter
        self.onlineclassifier.printStatus()
        #print selectedPatches,"doTrainstep for all patches %.2ffps" %(1./(time.time()-tic))
        return (selectedPatches, 1-(float(correctPatches)/numpy.max((selectedPatches,1))))

    def doEvalStep(self, data, mask, groundtruth=None):
        self.data_patcher.setImage(data.camImg)

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patchmask.fill(1)


        selectedPatches=0

        contextvec = self.contextutilisation.getContextVec(data);
        #print "Context: ", contextvec

        self.relmat = ReliabilityMatrix(self.data_patcher.getWidth(), self.data_patcher.getHeight(), startval=-1)
        for p in range(self.data_patcher.getNrOfPatches()):
            cp = self.data_patcher.getPatchByNr(p)
            (xp,yp)=self.data_patcher.getPosByNr(p)

            #isConfusable=1
            #if not groundtruth==None:
            #    isConfusable = groundtruth.isConfusable[xp, yp]
            #    if isConfusable<1:
            #        groundtruth.setReliability(xp,yp,[-1,-1,-1])
            #        self.relmat.setReliability(xp, yp, [-1,-1,-1])


            if (not cp==None): #(isConfusable>0) and
                selectedPatches+=1
                if self.featurewidth>0:
                    cp_transformed = self.featuretransformation.doTransformation(cp)
                else:
                    cp_transformed=[]
                data = numpy.hstack((cp_transformed, contextvec))
                #print "data:", data











                ##Offline classifier:
                result_offline = self.offlineclassifier.doEvalStep(cp_transformed)
                result_offline[0]*=0.007
                result_offline[1]*=0.007
                result_offline[2]*=0.007
                result_online = self.onlineclassifier.doEvalStep(data)


                comb_result = numpy.vstack((result_offline,result_online))
                win_classifiernr=0
                tmpmax = numpy.max(comb_result,1)
                if tmpmax[1]>tmpmax[0]:
                    win_classifiernr=1

                win_class=[0,0]
                tmpmax = result_offline
                if tmpmax[0]<tmpmax[1] and tmpmax[1]>tmpmax[2]:
                    win_class[0]=1
                elif tmpmax[0]<tmpmax[2]:
                    win_class[0]=2
                tmpmax = result_online
                if tmpmax[0]<tmpmax[1] and tmpmax[1]>tmpmax[2]:
                    win_class[1]=1
                elif tmpmax[0]<tmpmax[2]:
                    win_class[1]=2

                result=[0,0,0]
                result[win_class[win_classifiernr]]=1

                #print "online", result_online, " offline", result_offline, "erg", result




                self.relmat.setReliability(xp, yp, result)
            else:
                self.relmat.setReliability(xp, yp, [-1,-1,-1])
        #print "eigen", neigen, "Dist", dist


        return self.relmat

    def finishTrain(self):
        self.onlineclassifier.finishTrain()
        pass


    def visualize(self, visoptions=None, save=None):
        if not visoptions==None:
            if (visoptions.has_key("Classifier Result") and visoptions["Classifier Result"].rfind("true")>=0) or not save==None:
                self.axes[0].clear()
                self.relmat.visualize(axis=self.axes[0], pie=True)
                self.axes[0].set_title("Classifier Result")
            if (visoptions.has_key("Input Patches") and visoptions["Input Patches"].rfind("true")>=0) or not save==None:
                self.axes[1].clear()
                self.data_patcher.visualize(axis=self.axes[1])
                self.axes[1].set_title("Input Patches")
            if visoptions.has_key("PCA-Components") and visoptions["PCA-Components"].rfind("true")>=0:
                self.featuretransformation.visualize()

        self.fig.show()
        if not save==None:
            filename = save + "_" + self.getName() + ".png"
            self.fig.savefig(filename, transparent=True)



    def getVisOptions(self):
        return ["Classifier Result", "Input Patches", "Ground Truth", "Failed Patches", "PCA-Components"]

    def storeState(self, outfile):
        ptr = open(outfile, "wb" )
        self.onlineclassifier.storeState(ptr)
        ptr.close()


    def restoreState(self, infile):
        ptr = open(infile, "rb" )
        self.onlineclassifier.restoreState(ptr)
        ptr.close()

    def getExperimentInfo(self):
        res="Parameter:<ul>"

        for e in self.params:
            res += "<li>" + str(e) + " = " + str(self.params[e]) + "</li>"

        return res+"</lu></br>"

    def getName(self):
        return "online_gxlvq"

    def saveState(self, path):
        targetpath = path + "states/"
        if not os.path.exists(targetpath):
                    os.mkdir(targetpath)
        self.onlineclassifier.saveState(targetpath, self.traindata_len)









































class FeatureExtraction(object):

    params=None
    evecs=None
    evals=None
    mean_X=None

    evecs_cut=None

    eigenvectors=0

    def __init__(self, params):
        if not params.has_key("eigenvectors"): params["eigenvectors"]=5
        if not params.has_key("PCA_normalization"): params["PCA_normalization"]=False
        if not params.has_key("pca_data"): params["pca_data"]="/hri/localdisk/jqueisse/experiments/pca_estimator_patchsize5_run11/train_finish_cl.p"
        self.params = params

        self.eigenvectors = self.params["eigenvectors"]

        #Load config data
        self.restoreState(self.params["pca_data"], newversion=params["PCA_normalization"])

        print self.evecs.shape
        print self.evecs_cut.shape
        print self.evecs_cut
        print numpy.sum(self.evecs_cut, axis=0)
        print numpy.sum(self.evecs[:self.eigenvectors,:], axis=1)
        print self.evals



    def doTransformation(self, data):


        transformed = numpy.dot(numpy.transpose(self.evecs_cut),numpy.transpose(data.flatten())-self.mean_X)
        if len(self.transf_mean)>0:
            #variance 1 centered
            transformed-=self.transf_mean[:self.eigenvectors]
            transformed/=numpy.sqrt(self.transf_var[:self.eigenvectors])


        return transformed

    def doReconstruction(self, data):


        if len(self.transf_mean)>0:
            #variance 1 centered
            data*=numpy.sqrt(self.transf_var[:self.eigenvectors])
            data+=self.transf_mean[:self.eigenvectors]


        reconstructed=numpy.transpose(numpy.dot(self.evecs_cut,data))+self.mean_X

        return reconstructed


    def getParams(self):
        return self.params


    def restoreState(self, infile, newversion=False):
        ptr = open(self.params["pca_data"], "rb" )
        self.evecs = pickle.load( ptr )
        self.evals = pickle.load( ptr )
        self.mean_X = pickle.load( ptr )
        self.evecs_cut = scipy.array(self.evecs[:,:self.eigenvectors])


        if newversion==True:
            self.transf_mean = pickle.load( ptr )
            self.transf_max = pickle.load( ptr )
            self.transf_min = pickle.load( ptr )
            self.transf_var = pickle.load( ptr )
        else:
            self.transf_mean=[]
            self.transf_max=[]
            self.transf_min=[]
            self.transf_var=[]


        ptr.close()

    def visualize(self):
        fig=pyplot.figure()
        count = self.evecs.shape[1]
        evdims = numpy.ceil(numpy.sqrt(self.evecs.shape[0]/3))
        len = numpy.ceil(numpy.sqrt(count))

        for i in range(count):
            imgdata = self.evecs[:,i].reshape(evdims,evdims,3)

            ax = fig.add_subplot(len,len,i)
            ax.set_axis_off()
            ax.imshow(imgdata);
        fig.show()
















class PCAEstimator(ControlLogic):

    traindata_collector=None
    traindata_len=0

    patch_size=-1
    in_size=None

    data_patcher=None
    pcamodule=None



    def __init__(self, in_size=(256,256), patchsize=5, startstate=None, maxeigen=30):

        self.patch_size = patchsize
        self.in_size=in_size

        if not startstate==None:
            self.restoreState(startstate)

        self.data_patcher=ImagePatch(self.patch_size, self.in_size)

        self.traindata_collector=scipy.zeros((3000, (self.patch_size**2)*3))

        #self.pcamodule = IPCA( ((self.patch_size**2)*3), maxeigen)


    def doPca(self, X):


        #X= X[::2000,:]


        #num_data,dim = X.shape


        print X.shape



        mean_X = numpy.mean(X,axis=0)
        X -= mean_X


        print X.shape
        C = numpy.cov(numpy.transpose(X))

        evals,evecs = numpy.linalg.eig(C)
        indices = numpy.argsort(evals)
        indices = indices[::-1]
        evecs = evecs[:,indices]
        evals = evals[indices]


        #scaling - max min :
        transformed = numpy.dot(numpy.transpose(evecs),numpy.transpose(X))

        self.transf_mean = numpy.mean(transformed, axis=1)
        self.transf_max = numpy.max(transformed, axis=1)
        self.transf_min = numpy.min(transformed, axis=1)
        self.transf_var = numpy.var(transformed, axis=1)

        print self.transf_mean
        print self.transf_max
        print self.transf_min
        print self.transf_var





        print numpy.sum(evecs,axis=0)
        print numpy.sum(evecs,axis=1)



        return evecs,evals,mean_X

    def doTrainStep(self, data, mask):
        self.data_patcher.setImage(data.camImg)

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patches.fill(1)

        tic = time.time()

        if self.traindata_collector.shape[0]<self.traindata_len+self.data_patcher.getNrOfPatches():
            '''
            Allocate Mem ;)
            '''
            self.traindata_collector = scipy.vstack((self.traindata_collector, scipy.zeros((self.data_patcher.getNrOfPatches(),(self.patch_size**2)*3))))

        selectedPatches=0

        for p in range(self.data_patcher.getNrOfPatches()):
            cp = self.data_patcher.getPatchByNr(p)
            if not cp==None:
                #print numpy.matrix(cp.flatten()).transpose().shape
                #self.pcamodule.update(numpy.matrix(cp.flatten()).transpose())
                selectedPatches+=1
                self.traindata_collector[self.traindata_len,:]=cp.flatten()
                self.traindata_len +=1
                #self.traindata_collector = scipy.hstack((self.traindata_collector, cp.flatten()))

        print self.data_patcher.getNrOfPatches(),"doTrainstep for all patches %.2ffps" %(1./(time.time()-tic))
        return selectedPatches

    def doEvalStep(self, data, mask=None, neigen=10):
        self.data_patcher.setImage(data.camImg)

        if not mask==None:
            self.data_patcher.patchmask=mask
        else:
            self.data_patcher.patchmask.fill(1)

        dist = 0
        selectedPatches=0
        evecs = scipy.array(self.evecs[:,:neigen])
        for p in range(self.data_patcher.getNrOfPatches()):
            cp = self.data_patcher.getPatchByNr(p)
            if not cp==None:
                selectedPatches+=1
                transformed = numpy.dot(numpy.transpose(evecs),numpy.transpose(cp.flatten())-self.mean_X)
                reconstructed=numpy.transpose(numpy.dot(evecs,transformed))+self.mean_X
                dist += scipy.sqrt(scipy.sum((cp.flatten()-reconstructed)**2))

        #print "eigen", neigen, "Dist", dist
        difdist=0
        if selectedPatches>0:
            difdist = dist/selectedPatches



        return (difdist, selectedPatches)

    def finishTrain(self):

        self.traindata_collector = self.traindata_collector[:self.traindata_len,:]
        '''
        Compute PCA
        '''
        print self.traindata_collector.shape
        self.evecs,self.evals,self.mean_X = self.doPca(self.traindata_collector)


    def visualize(self, full=False):
        pass

    def storeState(self, outfile):
        ptr = open(outfile, "wb" )
        pickle.dump(self.evecs, ptr, pickle.HIGHEST_PROTOCOL)
        pickle.dump(self.evals, ptr, pickle.HIGHEST_PROTOCOL)
        pickle.dump(self.mean_X, ptr, pickle.HIGHEST_PROTOCOL)

        pickle.dump(self.transf_mean, ptr, pickle.HIGHEST_PROTOCOL)
        pickle.dump(self.transf_max, ptr, pickle.HIGHEST_PROTOCOL)
        pickle.dump(self.transf_min, ptr, pickle.HIGHEST_PROTOCOL)
        pickle.dump(self.transf_var, ptr, pickle.HIGHEST_PROTOCOL)


        #pickle.dump(self.traindata_collector, ptr, pickle.HIGHEST_PROTOCOL)
        #pickle.dump(self.traindata_len, ptr, pickle.HIGHEST_PROTOCOL)
        ptr.close()

    def restoreState(self, infile, newversion=False):
        print infile
        ptr=open(infile, "rb" )
        self.evecs = pickle.load( ptr )
        self.evals = pickle.load( ptr )
        self.mean_X = pickle.load( ptr )

        if newversion==True:
            self.transf_mean = pickle.load( ptr )
            self.transf_max = pickle.load( ptr )
            self.transf_min = pickle.load( ptr )
            self.transf_var = pickle.load( ptr )

        ptr.close()

    def getName(self):
        return "pca_estimator"

