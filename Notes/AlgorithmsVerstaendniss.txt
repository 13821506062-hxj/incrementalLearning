SVM:
-y=1,-1 

Functional margin: ˆγ(i) = y(i)(wT x + b)
ˆγ = min       ˆγ(i)
i=1,...,m

Geometric Margin 
γ(i) = y(i) (w/||w||)T x(i) + b/||w||

γ = min          γ(i)
i=1,...,m

if ||w|| = 1 then FunctionalMargin=Geometric-Margin

Functional Margin set to 1 (can be arbitary scaled)

maximizing functional margin as  1/||w|| is the same as minimizing ||w||

Convex optimization problem:
min   1/2 *||w||^2       (1)
,w,b
s.t. y(i)(wT x(i) + b) ≥ 1, i = 1, . . . ,m

Can be solved by optimization algorithm but transfering it to lagrangian duality enables efficient optimization.

Lagrangian to (1)

L(w, b, α) =1/2||w||2 − Sum_i=1-m [(αi y(i)(wT x(i) + b) − 1)]  (2)

derivative(L) wih respect to w 

w = Sum_i=1-m[αi y(i)x(i)]   (3)

derivative(L) wih respect to b 

Sum_i=1-m [αiy(i)] = 0

Primal is equal to dual solution as long as KKT conditions are true which is the case here therefore dual:

max W(α) = Sum_i=1-m [αi] − 1/2] Sum_i,j=1-m [y(i)y(j)αiαj<x(i), x(j)>

s.t. αi ≥ 0, i = 1, . . . ,m
     Sum_i=1-m[αiy(i)] =0

<x(i), xi> is inner product and is later replaced by Kernel-Function.

dual can be solved by SOM optimization, Dual is a convex problem, therefore otimization finds a global Maximum.

finding optimum αi then using them in (3) to determine w
finding b is straightforward see ng script.

Classifiying new example:

wT x + b = Sum_i=1-m[αiy(i)<x(i), xi> + b]




Soft Margin:

min     1/2||w||2 + C Sum_i=1-m[ξi]       ξi slack variables, C is the penalty, high C forces SVM to make less errors for training
,w,b
 s.t. y(i)(wT x(i) + b) ≥ 1 − ξi, i = 1, . . . ,m
  ξi ≥ 0, i = 1, . . . ,m.


Dual:
max W(α) = Sum_i=1-m [αi] − 1/2] Sum_i,j=1-m [y(i)y(j)αiαj<x(i), x(j)>

s.t. 0 ≤ αi ≤ C, i = 1, . . . ,m
     Sum_i=1-m[αiy(i)] =0

Convergence Criterias for optimization:
αi = 0 ⇒ y(i)(wT x(i) + b) ≥ 1     correct classification
αi = C ⇒ y(i)(wT x(i) + b) ≤ 1     error vectors, violating margin, not necessarily missclassified
0 < αi < C ⇒ y(i)(wT x(i) + b) = 1  support vectors


---------------------------------------------------------------------------------------------------------

Incremental/Decremental SVM:

adding incrementally samples considering set of MarginVectors S, Error Vectors E, Ignored Vectors R
whenever a sample is introduced the coefficients αi of S change to keep the KKT conditions. Therefore they are updated as well
as the term b. It can be that elements migrate between the Sets S, E and R depending on their αi.

Additionally an reserveVector Set which stores vectors of the ignored set having gi (derivative of W according to αi) <= eps can be stored.
These are nearly support vectors, which could be promoted to some after seeing the future data. Therefore The larger eps, 
the smaller the probability of missing a future margin or error vector in previous data.

Limit of the SVM complexity could be done as follows:
as soon as the limit of n support vectors/ error vectors is suprassed the support vector having the smallest αi could be decrementally removed. 


Paper from Diehl explains the math too, and provides somehow a efficient way to change C and sigma online. 


What has to be stored:
-support vectors
-error vectors
-coefficients
-Window of candidates(recent n samples)


----------------------------------------------------------------------------------

Random Forest:

-Ensemble of Deciscion Trees
-Each Tree is trained on a random subset of the complete training data. The performance on the untrained data is the out of bag - error (OOBE).
-Each Node performs evaluates a random subset of the completet features. For each of the features thresholds at the values of the samples are created. 
 Therefore a lot of tests are possible. The threshold with the feature giving the highest information gain (according to shannon entropy, a measure for uncertainty) is chosen.
-Is a the subset of features chosen per Tree or per Node???
-Result of tree is averaged over all trees. Deciscion tree is prone to noise since it is a high-variant classifier, this reduces the variance and delivers better results. 


---------------------------
Online Random Forest:
-As random Forest, but a predefined number of trees starts with only the rootNode, nodes are added incrementally(splitting of nodes).
-A node is splitted when it has seen a predefined number of samples, and a test performs better than a prefdefined threshold(not to find in the implementation)
-Each Node performs a number of random tests (random featureSubset, random Thresholds). The test which performs the best according to Gini-Index is chosen.
-Trees can be discarded from the set depending on a propability according to their OOBE(is proposed but not implemented). 

What has to be stored:
K classes
-Forest consists of n Trees
-Each Tree has a max depth of d, therefore max (2^d)-1 nodes, 2^(d-1) Leaves   d = 1 only root
-Each not-leaf-node sonsists of a Threshold + dimension
-Each Leaf stores the number of samples per class

n*(2^(d-1) * K + 2^(d-1)-1*(Tr+d))


For Splitting:
-Each Node consists of number of t random tests (Threshold + dimension)
-Each Test calculates the number of samples per class bigger than the threshold, the number of samples per class smaller than the threshold (needed to calculate Gini-Index)
-Best Test is chosen when splitted.


