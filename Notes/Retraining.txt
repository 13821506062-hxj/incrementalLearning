Retraining:
-Motivation aus offline GLVQ training in mehreren Epochen, Prototypen brauchen eine gewisse anzahl an gradient descent schritten um zu konvergieren. Erhoehen der Lernrate ist
keine Loesung da das zur Divergenz fuehren kann. Portieren dieses Effektes auf Online Setting.

Experiment Kuenstliches Beispiel, USPS random order:
-Effekt Prototypen brauchen weniger beispiele um sich genauer zu positionieren. Wichtig also fuer schnelle konvergenz. 
-Folglich bringt retraining mit zunehmeneder Anzahl von samples pro Klasse tendenziell weniger.

Experiment Kuenstliches Beispiel, USPS random order aber mit insertions:
-Besonders sinnvoll bei sparsem Modell (wenig prototypen), je weniger Prototypen desto wichtiger deren Position, Je mehr Prototypen desto geringer das Delta. 
-Durch zusaetzliche Insertions wird der Effekt geschmaelert, da neue Prototypen die Kostenfunktion ebenfalls senken. 

Experiment Kuenstliches Beispiel, USPS sequential order:
-LVQ anfaellig bei sequenzen von Datenbeispielen mit gleichen Labeln, deswegen Fensterfunktion bzw. Logistische funktion bei GLVQ. Trotzdem
ist der Effekt immer noch reproduzierbar auch wenn nur in abgeschwaechter Form.
-Durch random retraining wird das Training gegen lange Sequenzen robust. Sequenzen werden aufgebrochen durch zufaelliges Samplen. Divergenz-Effekt nicht vorhanden. Vorausgesetzt ist ein ausreichend grosses Fenster was Beispiele von mehreren Klassen enthaelt (Experiment Fenstergroesse Kuenstlicher Datensatz, USPS sequentiell)



-Insgesamt sinnvol da kaum mehraufwand
-gleiche/hoehere Accuracy bei geringerer/gleicher Komplexitaet, Besseres Model nach weniger Trainingsbeispielen - > weniger Fehler waehrend Training -> Fuer Prediction setting sehr gut...
-Deutlich robuster bei sequentiellem Training (nicht i.i.d verteilten Daten).


To-Do
Overfitting auf Fensterdaten vllt iwie nachstellen. 
Sequentiell effekt bei Chunks untersuchen.
Versuch mit verschiedenen Kuenstlichen Datensaetzen machen mit verschieden Sparsen Modells Tabelle(Feste inserierungsintervalle 0, 500, 250, 100) (sequentiell/nicht sequentiell).
       sequentiell, DSName
            | 16        | 30 | 50 | 100 | 
noTrain     | acc(+-std)
Train       |
Retrain1    |		 
Rertrain5   |
            |


Datensaetze:
USPS, COIL, OUTDOOR, DNA, LETTER, PENDIGITS, OptDigits (Datensaetze nachschlagen bei Safari im Paper)
